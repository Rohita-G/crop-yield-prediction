# Project Technical Documentation

## 1. Project Overview
**Goal**: Build a predictive model for US Corn Yields using historical weather and soil data.
**Approach**: 
1.  Ingest cleaned data (Weather + Soil + Yield).
2.  Preprocess (Scale, Clean, Split by Time).
3.  Train a Random Forest Regressor.
4.  Evaluate on unseen data (Year 2018).

---

## 2. Directory Structure & File Manifest

### `data/` (Data Storage)
*   **`raw/`**: Contains the source dataset.
    *   `khaki_multi_crop_yield.csv`: The primary dataset (25,306 rows).
    *   `usa-corn-belt-crop-yield.py`: Helper script from the original repo (reference).
*   **`processed/`**: Generated by `src/preprocessing.py`.
    *   `train_data.csv`: Cleaned training set (Years 1980-2017).
    *   `test_data.csv`: Cleaned testing set (Year 2018).

### `src/` (Source Code)
*   **`eda.py`**: **Exploratory Data Analysis**.
    *   *Function*: Loads raw data, prints statistics, and generates plots (`corn_yield_distribution.png`).
*   **`preprocessing.py`**: **Data Pipeline**.
    *   *Function*: Loads CSV, removes missing targets, selects features (`W_*`, Soil), splits data by year (<2018 vs 2018), and scales values. Saves processed CSVs.
*   **`model.py`**: **Machine Learning**.
    *   *Function*: Initializes a Random Forest, trains it on `train_data`, evaluates on `test_data`, and saves the model (`rf_model.pkl`).

### `results/` (Outputs)
*   `rf_model.pkl`: The trained model artifact.
*   `feature_importance.png`: Bar chart of top predictive features.
*   `predictions_vs_actual.png`: Scatter plot of model performance.

---

## 3. Data Flow Pipeline
1.  **Ingestion**: `data/raw/khaki_multi_crop_yield.csv` is loaded.
2.  **Cleaning**: Rows with missing `weather` or `yield` are dropped.
3.  **Feature Engineering**: 
    *   **Weather**: 300+ columns (`W_i_j`) representing weekly averages.
    *   **Soil**: Named columns (e.g., `clay_mean`).
    *   **Selection**: We use all available numeric features.
4.  **Splitting**: 
    *   **Training**: 1980 - 2017 (~24k samples).
    *   **Testing**: 2018 (~470 samples).
5.  **Modeling**: Random Forest (`n_estimators=50`) learns patterns in the training data.
6.  **Prediction**: Model predicts 2018 yield, compared against actuals.

---

## 4. How to Run
1.  **Install Requirements**:
    ```bash
    pip install -r requirements.txt
    ```
2.  **Run Analysis** (Optional):
    ```bash
    python src/eda.py
    ```
3.  **Train Model** (Runs preprocessing + training):
    ```bash
    python src/model.py
    ```
